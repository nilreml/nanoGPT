{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import torch\n",
    "from functorch.compile import aot_function, aot_module\n",
    "from torch import Tensor, nn\n",
    "from torch._subclasses import FakeTensorMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # type: ignore  # noqa: PGH003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.tensor([[-1, 1], [-2, 2]], dtype=dtype))\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        return torch.matmul(x, self.weights)\n",
    "\n",
    "    if TYPE_CHECKING:\n",
    "        def __call__(self, x:Tensor) -> Tensor: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor([[1, 2]], dtype=dtype)\n",
    "target = torch.tensor([[0, 1]], dtype=dtype)\n",
    "\n",
    "model = Layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -1.0\n",
      "tensor([[-5.,  5.]], grad_fn=<MmBackward0>)\n",
      "tensor([[-1.,  1.],\n",
      "        [-2.,  2.]])\n",
      "tensor([[1., 1.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "output = model(data)\n",
    "loss = (output - target).sum()\n",
    "print(f\"loss: {loss}\")\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(output)\n",
    "print(model.weights.data)\n",
    "print(model.weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2):\n",
      "    mm = torch.ops.aten.mm.default(primals_2, primals_1)\n",
      "    return [mm, primals_1, primals_2]\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "def forward(self, primals_1, primals_2, tangents_1):\n",
      "    t = torch.ops.aten.t.default(primals_2);  primals_2 = None\n",
      "    mm_1 = torch.ops.aten.mm.default(t, tangents_1);  t = None\n",
      "    t_1 = torch.ops.aten.t.default(primals_1);  primals_1 = None\n",
      "    mm_2 = torch.ops.aten.mm.default(tangents_1, t_1);  tangents_1 = t_1 = None\n",
      "    return [mm_1, mm_2]\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/merlin/.virtualenvs/nanogpt23/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:117: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# The compiler_fn is called after the forward and backward graphs are extracted.\n",
    "# Here, we just print the code in the compiler_fn. Return of this function is a callable.\n",
    "def compiler_fn(fx_module: torch.fx.GraphModule, _):\n",
    "    print(fx_module.code)\n",
    "    return fx_module\n",
    "\n",
    "# Pass on the compiler_fn to the aot_function API\n",
    "aot_print_fn = aot_module(model, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "#aot_print_fn = aot_function(model, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n",
    "\n",
    "# Run the aot_print_fn once to trigger the compilation and print the graphs\n",
    "cloned_inputs = data.clone().detach().requires_grad_(True)\n",
    "#with FakeTensorMode(allow_non_fake_inputs=True):\n",
    "res = aot_print_fn(cloned_inputs)\n",
    "res.sum().backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt3.12.3-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
