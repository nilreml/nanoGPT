{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no bias, rmsnorm\n",
    "\n",
    "class Config(BaseModel):\n",
    "        batch_size: int\n",
    "        seq_len: int\n",
    "        dim_model: int\n",
    "        dim_mlp: int\n",
    "        num_heads: int\n",
    "        num_layers: int\n",
    "        vocab_size: int\n",
    "\n",
    "def num_activations(config: Config) -> int:\n",
    "    return(\n",
    "        config.batch_size * config.seq_len * config.num_layers * (\n",
    "              2*config.dim_mlp + config.dim_model     # FFN\n",
    "            + 4*config.dim_model                      # RMSNorm x2\n",
    "            + 5*config.dim_model                      # SelfAttention non-head\n",
    "            + 2*config.dim_model                      # Residual FFN, SelfAttention\n",
    "            )\n",
    "        + config.batch_size * 4*config.seq_len**2 * config.num_heads                        # SelfAttention head\n",
    "        + config.batch_size * config.seq_len * (2*config.dim_model + 2*config.vocab_size)   # Embedding, Unembedding, Loss\n",
    "        + config.batch_size * config.seq_len * 2*config.dim_model                           # Final RMSNorm\n",
    "    )\n",
    "\n",
    "def num_weights(config: Config) -> int:\n",
    "    return(\n",
    "        config.num_layers * (\n",
    "                2*config.dim_model*config.dim_mlp      # FFN\n",
    "                + 4*config.dim_model*config.dim_model  # SelfAttention\n",
    "                + 2*config.dim_model                   # RMSNorm x2\n",
    "                )\n",
    "        + 2*config.dim_model*config.vocab_size         # Embedding, Unembedding\n",
    "        + config.dim_model                             # Final RMSNorm\n",
    "    )\n",
    "\n",
    "def num_bytes_per_activation(variant: Literal[\"conventional\", \"a\", \"b\", \"c\", \"d\"]) -> int:\n",
    "    match variant:\n",
    "        case \"conventional\":\n",
    "            bytes_per_value = 2\n",
    "        case \"a\":\n",
    "            bytes_per_value = 2\n",
    "        case \"b\":\n",
    "            bytes_per_value = 2\n",
    "        case \"c\":\n",
    "            bytes_per_value = 1\n",
    "        case \"d\":\n",
    "            bytes_per_value = 1\n",
    "    return bytes_per_value * 2  # 1 per activation, 1 per activation gradient\n",
    "\n",
    "def num_bytes_per_weight(variant: Literal[\"conventional\", \"a\", \"b\", \"c\", \"d\"]) -> int:\n",
    "    match variant:\n",
    "        case \"conventional\":\n",
    "            weight = 6\n",
    "            weight_grad = 2\n",
    "            moment1 = 4\n",
    "            moment2 = 4\n",
    "        case \"a\":\n",
    "            weight = 4\n",
    "            weight_grad = 2\n",
    "            moment1 = 2\n",
    "            moment2 = 2\n",
    "        case \"b\":\n",
    "            weight = 4\n",
    "            weight_grad = 2\n",
    "            moment1 = 2\n",
    "            moment2 = 0\n",
    "        case \"c\":\n",
    "            weight = 2\n",
    "            weight_grad = 1\n",
    "            moment1 = 1\n",
    "            moment2 = 1\n",
    "        case \"d\":\n",
    "            weight = 2\n",
    "            weight_grad = 1\n",
    "            moment1 = 1\n",
    "            moment2 = 0\n",
    "    return weight + weight_grad + moment1 + moment2\n",
    "\n",
    "def num_bytes(config: Config, variant: Literal[\"conventional\", \"a\", \"b\", \"c\", \"d\"]) -> int:\n",
    "    return (\n",
    "          num_activations(config) * num_bytes_per_activation(variant)\n",
    "        + num_weights(config) * num_bytes_per_weight(variant)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights            : 5.234 M\n",
      "activations        : 293.2 M\n",
      "memory weights     : 0.07799 GB\n",
      "memory activations : 1.092 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = Config(\n",
    "    batch_size = 128,\n",
    "    seq_len = 64,\n",
    "    dim_model = 256,\n",
    "    dim_mlp = 1024,\n",
    "    num_heads = 8,\n",
    "    num_layers = 6,\n",
    "    vocab_size = 1000,\n",
    ")\n",
    "variant = \"conventional\"\n",
    "\n",
    "print( f\"weights            : {num_weights(config)     / 1e6:.4} M\\n\"\n",
    "       f\"activations        : {num_activations(config) / 1e6:.4} M\\n\"\n",
    "       f\"memory weights     : {num_weights(config)     * num_bytes_per_weight(variant)     / 1024/1024/1024:.4} GB\\n\"\n",
    "       f\"memory activations : {num_activations(config) * num_bytes_per_activation(variant) / 1024/1024/1024:.4} GB\\n\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
